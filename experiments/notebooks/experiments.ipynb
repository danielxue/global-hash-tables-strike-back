{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Experiments",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Common",
   "id": "51c6f0cbcdd577f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "markers_cycle = ['o', '^', 'D', '*', 'P']"
   ],
   "id": "128309b892b301dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_df(df, title, colors, markers, ax=None, **kwargs):\n",
    "    for (col, marker, color) in zip(df.columns, markers, colors):\n",
    "        ax = df[col].plot(\n",
    "            title=title.capitalize(),\n",
    "            ax=ax,\n",
    "            c=color,\n",
    "            marker=marker,\n",
    "            markersize={'*': 8, 'P': 7}.get(marker, 6),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    ax.grid(True, linestyle='--')\n",
    "    return ax\n",
    "\n",
    "def plot_dfs(dfs, subtitles, title, dim, axs=None, colors=None, markers=None, figsize=None, **kwargs):\n",
    "    if colors is None:\n",
    "        colors = colors_cycle[:len(dfs[0].columns)]\n",
    "    if markers is None:\n",
    "        markers = [''] * len(dfs[0].columns)\n",
    "    elif markers is True:\n",
    "        markers = markers_cycle[:len(dfs[0].columns)]\n",
    "    if figsize is None:\n",
    "        figsize = (6.4 * dim[1], 4.8 * dim[0])\n",
    "    if axs is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        axs = fig.subplots(\n",
    "            nrows=dim[0],\n",
    "            ncols=dim[1],\n",
    "            sharex=True,\n",
    "            sharey=False,\n",
    "        ).flatten()\n",
    "        fig.suptitle(title.capitalize())\n",
    "        fig.set_dpi(300)\n",
    "    for (df, subtitle, ax) in zip(dfs, subtitles, axs):\n",
    "        plot_df(df, title=subtitle, ax=ax, markers=markers, colors=colors, legend=False, **kwargs)\n",
    "    handles, labels = axs.flatten()[0].get_legend_handles_labels()\n",
    "    axs.flatten()[0].legend(handles, labels, loc='upper left')\n",
    "    return axs"
   ],
   "id": "255dfbd39fb781c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_df(path):\n",
    "    raw_df = pd.read_csv(path)\n",
    "\n",
    "    raw_df['type'] = raw_df['workload'].str[-3:]\n",
    "    raw_df['workload'] = raw_df['workload'].str[:-3].replace({\n",
    "        'Folklore': 'Folklore*',\n",
    "        'FineGrainedLocking': 'Locking',\n",
    "        'Leap': 'Leapfrog',\n",
    "        'Partitioned': 'Partition',\n",
    "        'ThreadLocal': 'Thread Local'\n",
    "    })\n",
    "\n",
    "    raw_df['cardinality'] = (\n",
    "        (raw_df['keys'] >= raw_df['elements']) * 2\n",
    "        + ((raw_df['keys'] < raw_df['elements']) & (raw_df['keys'] > 10_000)) * 1\n",
    "    ).replace({2: 'Unique', 1: 'High', 0: 'Low'})\n",
    "    raw_df['distribution'] = (((raw_df['zipf'] > 0) * 2) + ((raw_df['heavy hitter'] > 0) * 1)).replace({2: 'Zipf', 1: 'Heavy Hitter', 0: 'Uniform'})\n",
    "\n",
    "    raw_df['latency'] = raw_df[['t1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9']].replace(0, np.nan).median(axis=1)\n",
    "    raw_df['throughput'] = raw_df['elements'] / raw_df['latency']\n",
    "    raw_df['time per element'] = raw_df['latency'] / raw_df['elements']\n",
    "    raw_df['throughput per thread'] = raw_df['throughput'] / raw_df['threads']\n",
    "\n",
    "    return raw_df"
   ],
   "id": "d773459ca38a4cff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "THREADS = [1, 2, 4, 8, 16, 32]\n",
    "THREADS_LABELS = [1, '', 4, 8, 16, 32]"
   ],
   "id": "a3ca3c10795ec4f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scaling Experiment",
   "id": "48928bd19ca36a4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw_scaling_df = load_df('../data/scaling_experiment.csv')\n",
    "scaling_df = pd.pivot_table(raw_scaling_df, values=['latency', 'throughput', 'throughput per thread', 'time per element'], index=['workload', 'cardinality', 'distribution', 'type'], columns='threads')\n",
    "for thread in THREADS:\n",
    "    scaling_df[('speedup', thread)] = scaling_df[('latency', 1)] / scaling_df[('latency', thread)]"
   ],
   "id": "1ddf2d8b44f6a74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_scaling_df(df, colors=None, markers=True):\n",
    "    dim = (2, 5)\n",
    "    linewidth = 2\n",
    "    fig = plt.figure(figsize=(2.4 * dim[1], 3.2 * dim[0]), layout='constrained')\n",
    "    fig.set_dpi(300)\n",
    "    axs = fig.subplots(\n",
    "        nrows=dim[0],\n",
    "        ncols=dim[1],\n",
    "        sharex=True,\n",
    "        sharey=False,\n",
    "    )\n",
    "    for ax in axs.flatten()[dim[1]:]:\n",
    "        ax.plot(THREADS, THREADS, label='Ideal', linestyle='--', color='black', linewidth=linewidth)\n",
    "\n",
    "    plot_dfs([\n",
    "        df.xs('Uniform', level=2).xs('Low', level=1).T.loc['throughput'],\n",
    "        df.xs('Uniform', level=2).xs('High', level=1).T.loc['throughput'],\n",
    "        df.xs('Zipf', level=2).xs('High', level=1).T.loc['throughput'],\n",
    "        df.xs('Heavy Hitter', level=2).xs('High', level=1).T.loc['throughput'],\n",
    "        df.xs('Uniform', level=2).xs('Unique', level=1).T.loc['throughput'],\n",
    "    ], [\n",
    "        'low cardinality (uniform)',\n",
    "        'high cardinality (uniform)',\n",
    "        'high cardinality (zipfian)',\n",
    "        'high cardinality (heavy hitter)',\n",
    "        'unique keys (uniform)',\n",
    "    ],\n",
    "        None,\n",
    "        (1, dim[1]),\n",
    "        axs=axs[0],\n",
    "        colors=colors,\n",
    "        markers=markers,\n",
    "        xlabel='threads'.capitalize(),\n",
    "        ylabel='throughput (elements/sec)'.capitalize(),\n",
    "        logx=False,\n",
    "        logy=False,\n",
    "        linewidth=linewidth,\n",
    "    )\n",
    "    plot_dfs([\n",
    "        df.xs('Uniform', level=2).xs('Low', level=1).T.loc['speedup'],\n",
    "        df.xs('Uniform', level=2).xs('High', level=1).T.loc['speedup'],\n",
    "        df.xs('Zipf', level=2).xs('High', level=1).T.loc['speedup'],\n",
    "        df.xs('Heavy Hitter', level=2).xs('High', level=1).T.loc['speedup'],\n",
    "        df.xs('Uniform', level=2).xs('Unique', level=1).T.loc['speedup'],\n",
    "    ], [\n",
    "        'low cardinality (uniform)',\n",
    "        'high cardinality (uniform)',\n",
    "        'high cardinality (zipfian)',\n",
    "        'high cardinality (heavy hitter)',\n",
    "        'unique keys (uniform)',\n",
    "    ],\n",
    "        None,\n",
    "        (1, dim[1]),\n",
    "        axs=axs[1],\n",
    "        colors=colors,\n",
    "        markers=markers,\n",
    "        xlabel='threads'.capitalize(),\n",
    "        ylabel='speedup'.capitalize(),\n",
    "        logx=False,\n",
    "        logy=False,\n",
    "        linewidth=linewidth,\n",
    "    )\n",
    "\n",
    "    for (i, ax) in enumerate(axs.flatten()):\n",
    "        ax.set_xticks(THREADS, THREADS_LABELS)\n",
    "        ax.set_xticks([], [], minor=True)\n",
    "        if i >= dim[1]:\n",
    "            ax.set_title('')\n",
    "        else:\n",
    "            print(ax.yaxis.offsetText)\n",
    "        if i % dim[1] != 0:\n",
    "            ax.set_ylabel('')"
   ],
   "id": "56594edc5bf9a5ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Graphs",
   "id": "80ba7375b63b7d40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ticketing_scaling_df = scaling_df.xs('Map', level=3).T[['Cuckoo', 'Dash', 'Folklore*', 'Iceberg', 'Leapfrog']].T\n",
    "\n",
    "plot_scaling_df(ticketing_scaling_df)\n",
    "plt.savefig('../figures/ticketing_scaling.svg')"
   ],
   "id": "babc6423a9534366",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pau_scaling_df = scaling_df.xs('Pau', level=3).T[['Atomic', 'Locking', 'Thread Local']].T\n",
    "\n",
    "plot_scaling_df(pau_scaling_df)\n",
    "plt.savefig('../figures/update_scaling.svg') "
   ],
   "id": "8021037bd6f16247",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "end_to_end_df = scaling_df.xs('E2E', level=3).T[['Atomic', 'Thread Local', 'Partition']].T\n",
    "\n",
    "plot_scaling_df(end_to_end_df, colors=(colors_cycle[0:1] + colors_cycle[2:4]), markers=(markers_cycle[0:1] + markers_cycle[2:4]))\n",
    "plt.savefig('../figures/e2e_scaling.svg')"
   ],
   "id": "e8ff0bd059ec224d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Speedup Table",
   "id": "a5719083237e6aa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for card in ['Low', 'High', 'Unique']:\n",
    "    print('\\\\midrule')\n",
    "    for dist in ['Uniform', 'Zipf', 'Heavy Hitter']:\n",
    "        part_lat = end_to_end_df.loc['Partition', card, dist]['latency']\n",
    "        atomic_lat = end_to_end_df.loc['Atomic', card, dist]['latency']\n",
    "        tl_lat = end_to_end_df.loc['Thread Local', card, dist]['latency']\n",
    "        vals = [\n",
    "            part_lat[1] / atomic_lat[1],\n",
    "            part_lat[1] / tl_lat[1],\n",
    "            part_lat[4] / atomic_lat[4],\n",
    "            part_lat[4] / tl_lat[4],\n",
    "            part_lat[16] / atomic_lat[16],\n",
    "            part_lat[16] / tl_lat[16],\n",
    "            part_lat[32] / atomic_lat[32],\n",
    "            part_lat[32] / tl_lat[32],\n",
    "        ]\n",
    "        val_strs = []\n",
    "        for val in vals:\n",
    "            if val >= 0.9 and val <= 1.1:\n",
    "                val_strs.append('\\\\underline{}{:.2f}{}'.format('{', val, '}'))\n",
    "            elif val > 1.1:\n",
    "                val_strs.append('\\\\textbf{}{:.2f}{}'.format('{', val, '}'))\n",
    "            else:\n",
    "                val_strs.append('{:.2f}'.format(val))\n",
    "        print('{} ({}) & {} & {} & {} & {} & {} & {} & {} & {} \\\\\\\\'.format(\n",
    "            {'Low' : 'Low cardinality', 'High': 'High cardinality', 'Unique': 'Unique keys'}[card] ,\n",
    "            {'Uniform' : 'uniform', 'Zipf': 'zipfian', 'Heavy Hitter': 'heavy hitter'}[dist],\n",
    "            val_strs[0],\n",
    "            val_strs[1],\n",
    "            val_strs[2],\n",
    "            val_strs[3],\n",
    "            val_strs[4],\n",
    "            val_strs[5],\n",
    "            val_strs[6],\n",
    "            val_strs[7],\n",
    "        ))"
   ],
   "id": "71cec9f5bb7fc09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fuzzy Ticketing Ablation",
   "id": "c7001a946322f973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fuzzy_df = load_df('../data/fuzzy_ticketing_experiment.csv')\n",
    "\n",
    "dim = (1, 2)\n",
    "fig = plt.figure(figsize=(2.4 * dim[1], 3.2 * dim[0]), layout='constrained')\n",
    "fig.set_dpi(300)\n",
    "axs = fig.subplots(\n",
    "    nrows=dim[0],\n",
    "    ncols=dim[1],\n",
    ")\n",
    "axs[0].bar(['Atomic', 'Fuzzy'], [\n",
    "    fuzzy_df[(fuzzy_df['workload'] == 'FolkloreUnfuzzy') & (fuzzy_df['keys'] == 1000)].reset_index().loc[0]['latency'],\n",
    "    fuzzy_df[(fuzzy_df['workload'] == 'Folklore*') & (fuzzy_df['keys'] == 1000)].reset_index().loc[0]['latency']\n",
    "])\n",
    "axs[0].set_ylabel('Latency (sec)')\n",
    "axs[0].set_title('Low cardinality (uniform)')\n",
    "axs[0].grid(axis='y')\n",
    "\n",
    "axs[1].bar(['Atomic', 'Fuzzy'], [\n",
    "    fuzzy_df[(fuzzy_df['workload'] == 'FolkloreUnfuzzy') & (fuzzy_df['keys'] == 10000000)].reset_index().loc[0]['latency'],\n",
    "    fuzzy_df[(fuzzy_df['workload'] == 'Folklore*') & (fuzzy_df['keys'] == 10000000)].reset_index().loc[0]['latency']\n",
    "])\n",
    "axs[1].set_title('High cardinality (uniform)')\n",
    "axs[1].grid(axis='y')\n",
    "\n",
    "plt.savefig('../figures/fuzzy_ticketer.svg')"
   ],
   "id": "74acc07a4a995800",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Latency Breakdown Ablation",
   "id": "f80c540f4b548680"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adapted from https://stackoverflow.com/questions/22787209/how-to-have-clusters-of-stacked-bars\n",
    "\n",
    "def plot_clustered_stacked(dfall, labels, title, ax, idx,**kwargs):\n",
    "    n_df = len(dfall)\n",
    "    n_col = len(dfall[0].columns)\n",
    "    n_ind = len(dfall[0].index)\n",
    "\n",
    "    for df in dfall : # for each data frame\n",
    "        ax = df.plot(kind='bar',\n",
    "                      linewidth=0,\n",
    "                      stacked=True,\n",
    "                      ax=ax,\n",
    "                      legend=False,\n",
    "                      grid=False,\n",
    "                      **kwargs)  # make bar plots\n",
    "\n",
    "    minor_ticks = []\n",
    "    h,l = ax.get_legend_handles_labels() # get the handles we want to modify\n",
    "    for i in range(0, n_df * n_col, n_col): # len(h) = n_col * n_df\n",
    "        for j, pa in enumerate(h[i:i+n_col]):\n",
    "            for rect in pa.patches: # for each index\n",
    "                x = rect.get_x() + 1 / float(n_df + 1) * i / float(n_col) - 1 / float(n_df + 1) - 0.04\n",
    "                w = 1 / float(n_df + 1)\n",
    "                rect.set_x(x)\n",
    "                rect.set_width(w)\n",
    "                minor_ticks += [x + w / 2]\n",
    "    minor_ticks = sorted(set(minor_ticks))\n",
    "    major_ticks = []\n",
    "    for i in range(n_ind):\n",
    "        mid = n_df // 2 - 1\n",
    "        left = minor_ticks[n_df * i + mid]\n",
    "        right = minor_ticks[n_df * i + mid + 1]\n",
    "        major_ticks += [(left + right) / 2]\n",
    "    ax.tick_params(axis='x', which='major', length=0)\n",
    "    ax.set_xticks(major_ticks, labels=dfall[0].index, rotation=0)\n",
    "    ax.set_xticks(minor_ticks, labels=np.tile(labels, n_ind), minor=True, rotation=0)\n",
    "    ax.set_yticks([0, 0.25, 0.5, 0.75, 1], labels=['0%', '25%', '50%', '75%', '100%'], rotation=0)\n",
    "    ax.set_title(title)\n",
    "    ax.margins(x=0, y=0)\n",
    "    for t in ax.get_xticklabels(minor=False):\n",
    "        t.set_y(-0.12)\n",
    "    if idx == -3:\n",
    "        l1 = ax.legend(h[:n_col], list(map(lambda l: l.capitalize(), l[:n_col])), loc='lower right')"
   ],
   "id": "40da63855757dabd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw_breakdown_df = load_df('../data/scaling_experiment.csv')\n",
    "raw_breakdown_df = raw_breakdown_df[raw_breakdown_df['type'] == 'E2E']\n",
    "total_time = raw_breakdown_df['initialization'] + raw_breakdown_df['ticketing'] + raw_breakdown_df['update'] + raw_breakdown_df['materialization']\n",
    "raw_breakdown_df['initialization'] /= total_time\n",
    "raw_breakdown_df['ticketing'] /= total_time\n",
    "raw_breakdown_df['update'] /= total_time\n",
    "raw_breakdown_df['materialization'] /= total_time\n",
    "breakdown_df = pd.pivot_table(raw_breakdown_df, values=['initialization', 'ticketing', 'update', 'materialization'],\n",
    "                              index=['workload', 'type', 'cardinality', 'distribution'], columns=['threads'],\n",
    "                              sort=False).T[['Atomic', 'Thread Local']].T"
   ],
   "id": "60f5f2125971402",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dim = (3, 1)\n",
    "fig = plt.figure(figsize=(4.5, 6.0), layout='constrained')\n",
    "axs = fig.subplots(\n",
    "    nrows=dim[0],\n",
    "    ncols=dim[1],\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    ").flatten()\n",
    "fig.set_dpi(300)\n",
    "\n",
    "for df, subtitle, ax in zip([\n",
    "    breakdown_df.xs('Uniform', level=3).xs('Low', level=2).xs('E2E', level=1).T,\n",
    "    breakdown_df.xs('Uniform', level=3).xs('High', level=2).xs('E2E', level=1).T,\n",
    "    breakdown_df.xs('Uniform', level=3).xs('Unique', level=2).xs('E2E', level=1).T,\n",
    "], [\n",
    "    'low cardinality (uniform)',\n",
    "    'high cardinality (uniform)',\n",
    "    'unique keys (uniform)',\n",
    "], axs):\n",
    "    plot_clustered_stacked([\n",
    "        df.xs(thread, level=1).T for thread in THREADS\n",
    "    ], THREADS, subtitle.capitalize(), ax, list(axs).index(ax) - len(axs))\n",
    "axs.flatten()[dim[0] - 1].set_xlabel('')\n",
    "plt.savefig('../figures/breakdown.svg')"
   ],
   "id": "6d9bde6214edb707",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Resizing Ablation",
   "id": "5dbef99aa686247d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw_resizing_df = load_df('../data/resizing_experiment.csv')\n",
    "raw_resizing_df['resized'] = raw_resizing_df['capacity'] < raw_resizing_df['keys']\n",
    "resizing_df = pd.pivot_table(raw_resizing_df, values=['throughput', 'latency'], index=['workload', 'cardinality', 'resized'], columns=['threads']).T[['Atomic', 'Thread Local', 'Partition']].T"
   ],
   "id": "ab101a0a4bf1c862",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dim = (1, 2)\n",
    "linewidth = 2\n",
    "fig = plt.figure(figsize=(2.4 * dim[1], 3.2 * dim[0]), layout='constrained')\n",
    "fig.set_dpi(300)\n",
    "axs = fig.subplots(\n",
    "    nrows=dim[0],\n",
    "    ncols=dim[1],\n",
    ")\n",
    "plot_dfs([\n",
    "        resizing_df.xs(False, level=2).xs('High', level=1).T.loc['throughput'],\n",
    "        resizing_df.xs(False, level=2).xs('Unique', level=1).T.loc['throughput'],\n",
    "    ], [\n",
    "        'high cardinality',\n",
    "        'unique keys',\n",
    "    ],\n",
    "    '',\n",
    "    (1, 2),\n",
    "    figsize=(3.0 * dim[1], 4.0 * dim[0]),\n",
    "    axs=axs,\n",
    "    colors=(colors_cycle[0:1] + colors_cycle[2:4]),\n",
    "    markers=(markers_cycle[0:1] + markers_cycle[2:4]),\n",
    "    xlabel='threads'.capitalize(),\n",
    "    ylabel='throughput'.capitalize(),\n",
    "    linewidth=linewidth,\n",
    "    linestyle='--',\n",
    ")\n",
    "plot_dfs([\n",
    "        resizing_df.xs(True, level=2).xs('High', level=1).T.loc['throughput'],\n",
    "        resizing_df.xs(True, level=2).xs('Unique', level=1).T.loc['throughput'],\n",
    "    ], [\n",
    "        'high cardinality',\n",
    "        'unique keys',\n",
    "    ],\n",
    "    '',\n",
    "    (1, 2),\n",
    "    axs = axs,\n",
    "    figsize=(3.2 * dim[1], 4.2 * dim[0]),\n",
    "    colors=(colors_cycle[0:1] + colors_cycle[2:4]),\n",
    "    markers=(markers_cycle[0:1] + markers_cycle[2:4]),\n",
    "    xlabel='threads'.capitalize(),\n",
    "    ylabel='throughput'.capitalize(),\n",
    "    linewidth=linewidth,\n",
    ")\n",
    "for (i, ax) in enumerate(axs.flatten()):\n",
    "    ax.set_xticks(THREADS, THREADS_LABELS)\n",
    "    ax.set_xticks([], [], minor=True)\n",
    "    if i % dim[1] != 0:\n",
    "        ax.set_ylabel('')\n",
    "handles, labels = axs.flatten()[0].get_legend_handles_labels()\n",
    "axs.flatten()[0].legend(handles[:3], labels[:3], loc='upper left')\n",
    "plt.savefig('../figures/resizing.svg')"
   ],
   "id": "a7f60ece49667807",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Memory Ablation",
   "id": "4b650e7f80a41557"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw_memory_df = pd.read_csv('../data/memory_experiment.csv')\n",
    "\n",
    "raw_memory_df['workload'] = raw_memory_df['workload'].str[:-3].replace({\n",
    "    'Folklore': 'Folklore*',\n",
    "    'FineGrainedLocking': 'Locking',\n",
    "})\n",
    "\n",
    "raw_memory_df['cardinality'] = (\n",
    "    (raw_memory_df['keys'] >= raw_memory_df['elements']) * 2\n",
    "    + ((raw_memory_df['keys'] < raw_memory_df['elements']) & (raw_memory_df['keys'] > 10_000)) * 1\n",
    ").replace({2: 'Unique', 1: 'High', 0: 'Low'})\n",
    "\n",
    "memory_df = pd.pivot_table(raw_memory_df, values=['memory'], index=['workload', 'cardinality'], columns='threads')\n",
    "\n",
    "for card in ['Low', 'High', 'Unique']:\n",
    "    vals = [\n",
    "        memory_df.loc['Atomic', card]['memory'][1],\n",
    "        memory_df.loc['ThreadLocal', card]['memory'][1],\n",
    "        memory_df.loc['Partitioned', card]['memory'][1],\n",
    "        memory_df.loc['Atomic', card]['memory'][8],\n",
    "        memory_df.loc['ThreadLocal', card]['memory'][8],\n",
    "        memory_df.loc['Partitioned', card]['memory'][8],\n",
    "        memory_df.loc['Atomic', card]['memory'][32],\n",
    "        memory_df.loc['ThreadLocal', card]['memory'][32],\n",
    "        memory_df.loc['Partitioned', card]['memory'][32],\n",
    "    ]\n",
    "    val_strs = []\n",
    "    for val in vals:\n",
    "        val_strs.append('{:.3f}'.format(val))\n",
    "\n",
    "    print('{} & {} & {} & {} & {} & {} & {} & {} & {} & {} \\\\\\\\'.format(\n",
    "        {'Low' : 'Low cardinality', 'High': 'High cardinality', 'Unique': 'Unique keys'}[card] ,\n",
    "        val_strs[0],\n",
    "        val_strs[1],\n",
    "        val_strs[2],\n",
    "        val_strs[3],\n",
    "        val_strs[4],\n",
    "        val_strs[5],\n",
    "        val_strs[6],\n",
    "        val_strs[7],\n",
    "        val_strs[8],\n",
    "    ))"
   ],
   "id": "45cb56489b9c656f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
